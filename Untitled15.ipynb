{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNei+/1uwmeU0yH57sdIzPu",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mounibnasr45/AmiAgri.github.io/blob/main/Untitled15.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "v0z5YG-_xXcJ"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import sentencepiece as spm\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "# Load data from article and summary directories\n",
        "def load_data(article_dir, summary_dir):\n",
        "    articles, summaries = [], []\n",
        "    for filename in os.listdir(article_dir):\n",
        "        article_path = os.path.join(article_dir, filename)\n",
        "        summary_path = os.path.join(summary_dir, filename)\n",
        "\n",
        "        if os.path.exists(article_path) and os.path.exists(summary_path):\n",
        "            # Use 'ISO-8859-1' to avoid decoding errors\n",
        "            with open(article_path, 'r', encoding='ISO-8859-1') as f:\n",
        "                article = f.read().strip()\n",
        "            with open(summary_path, 'r', encoding='ISO-8859-1') as f:\n",
        "                summary = f.read().strip()\n",
        "\n",
        "            if article and summary:\n",
        "                articles.append(article)\n",
        "                summaries.append(summary)\n",
        "\n",
        "    return articles, summaries\n",
        "\n",
        "articles, summaries = load_data(\n",
        "    r'/content/data/articles',\n",
        "    r'/content/data/summarize'\n",
        ")\n",
        "\n",
        "# Load SentencePiece model for tokenization\n",
        "sp = spm.SentencePieceProcessor(model_file=r'/content/en_32k.sentencepiece')\n",
        "\n",
        "# Update the max_length parameter in the tokenize function\n",
        "def tokenize(text, max_length=256):\n",
        "    # Encode the text using SentencePiece\n",
        "    tokenized_input = sp.encode(text, out_type=int)\n",
        "\n",
        "    # Create a mask (1 for actual tokens, 0 for padding)\n",
        "    input_length = len(tokenized_input)\n",
        "    input_mask = np.ones(input_length, dtype=int)  # Initialize mask with 1s\n",
        "\n",
        "    # Add padding if necessary\n",
        "    if input_length < max_length:\n",
        "        padding_length = max_length - input_length\n",
        "        tokenized_input = np.pad(tokenized_input, (0, padding_length), 'constant', constant_values=0)\n",
        "        input_mask = np.pad(input_mask, (0, padding_length), 'constant', constant_values=0)\n",
        "\n",
        "    return tokenized_input, input_mask\n",
        "def detokenize(ids):\n",
        "    return sp.decode(ids)\n",
        "# Encode articles and summaries with the updated max_length\n",
        "encoded_articles = [tokenize(article) for article in articles]\n",
        "encoded_summaries = [tokenize(summary) for summary in summaries]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to pad sequences to a fixed length\n",
        "def pad_sequences(sequences, max_length):\n",
        "    padded_sequences = []\n",
        "    for seq in sequences:\n",
        "        if len(seq) < max_length:\n",
        "            # Pad with zeros if sequence is shorter than max_length\n",
        "            padded_seq = np.pad(seq, (0, max_length - len(seq)), mode='constant', constant_values=0)\n",
        "        else:\n",
        "            # Truncate the sequence if it's longer than max_length\n",
        "            padded_seq = seq[:max_length]\n",
        "        padded_sequences.append(padded_seq)\n",
        "    return np.array(padded_sequences)\n",
        "\n",
        "# Convert the data into a Dataset object\n",
        "def create_tf_dataset(articles, summaries, batch_size=32, buffer_size=10000, max_length=256):\n",
        "    # Convert lists to tensors with padding\n",
        "    input_ids = pad_sequences([article[0] for article in articles], max_length)\n",
        "    input_masks = pad_sequences([article[1] for article in articles], max_length)\n",
        "    target_ids = pad_sequences([summary[0] for summary in summaries], max_length)\n",
        "    target_masks = pad_sequences([summary[1] for summary in summaries], max_length)\n",
        "\n",
        "    # Create a dataset from the tokenized inputs and summaries\n",
        "    dataset = tf.data.Dataset.from_tensor_slices(((input_ids, input_masks), (target_ids, target_masks)))\n",
        "\n",
        "    # Shuffle and batch the dataset\n",
        "    dataset = dataset.shuffle(buffer_size).batch(batch_size).prefetch(tf.data.experimental.AUTOTUNE)\n",
        "\n",
        "    return dataset\n",
        "\n",
        "# Example usage\n",
        "batch_size = 64\n",
        "max_length = 256  # Set the maximum length for padding\n",
        "dataset = create_tf_dataset(encoded_articles, encoded_summaries, batch_size=batch_size, max_length=max_length)\n",
        "\n",
        "# Display one line of the data\n",
        "for data in dataset.take(1):\n",
        "    (input_data, target_data) = data\n",
        "    input_ids, input_masks = input_data\n",
        "    target_ids, target_masks = target_data\n",
        "\n",
        "    # Display the first line of input and target data\n",
        "    print(\"Input IDs:\", input_ids.numpy()[0])   # First line of input ids\n",
        "    print(\"Input Masks:\", input_masks.numpy()[0])   # First line of input masks\n",
        "    print(\"Target IDs:\", target_ids.numpy()[0])  # First line of target ids\n",
        "    print(\"Target Masks:\", target_masks.numpy()[0])  # First line of target masks\n",
        "    break\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mXoNymovx5Fu",
        "outputId": "9331351a-fb04-4007-f9de-46888d211014"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input IDs: [ 1290    63     9  2870     7   223    21  2557  2233 19783  1290    63\n",
            "     9  1632     8   166   388    12  4234 11220     8 23940  2384  2233\n",
            "    57 16201   662    18   715     7   804   343  2180  7923    29  8642\n",
            "  3441  8237     3 25342     3 25618   489  5783 13649 16936   137    37\n",
            " 24146    52    26   258 13207   112     3     2 19853   357 16235  6441\n",
            "   540    12  4956  2231    21     8  8926    13     8  6578 28583     5\n",
            "    37   804    47   885  1019     6    28 12806   511  6677  8642  3441\n",
            "  8237     3    18     8  3888  4668     3    18  6733    91     8   166\n",
            "   356     5   299  1290    63     9   808     8   511   356     6   274\n",
            "     3  4076  4733   223    45     3 24279   323    16     8  2204    52\n",
            "    12  2054    11  1369     3     9  6177    18 14577     5  1290    63\n",
            "     9  5899 15627    10    96   196    31    51     3 28004    82  6441\n",
            "   540    45    48  5892    21     8 28583  8926     6  6055    34   656\n",
            "   128  1750    12    70  1342     5   978  6275  6963 15387     7    57\n",
            "   662   119  1508    12 13213    70  6441   540    45     8  5892     6\n",
            "    84   877  2177   227     8  2208  1882  6912    84    65    78   623\n",
            "  7760    44   709   209  9286  1342     5 20190  3441  2837   243     3\n",
            "    88   133 13213    46    73  7576  3676   866    21  4956   161    16\n",
            " 10508     6   213    72   145     3  5898   151   130  4792     5    37\n",
            "     3 26758    65  2162    24    34   133  4139     8   837  3229 24338\n",
            "  5892  2572    12   597   867    89    31     7  3583  4956  3069    16\n",
            " 22503  1823  1259     5]\n",
            "Input Masks: [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
            "Target IDs: [   37     3 26758    65  2162    24    34   133  4139     8   837  3229\n",
            " 24338  5892  2572    12   597   867    89    31     7  3583  4956  3069\n",
            "    16 22503  1823  1259     5   329    32    63     9  5899 15627    10\n",
            "    96   196    31    51     3 28004    82  6441   540    45    48  5892\n",
            "    21     8 28583  8926     6  6055    34   656   128  1750    12    70\n",
            "  1342     5   634 24146    52    26   258 13207   112     3     2 19853\n",
            "   357 16235  6441   540    12  4956  2231    21     8  8926    13     8\n",
            "  6578 28583     5 12146     7  6275  6963 15387     7    57   662   119\n",
            "  1508    12 13213    70  6441   540    45     8  5892     6    84   877\n",
            "  2177   227     8  2208  1882  6912    84    65    78   623  7760    44\n",
            "   709   209  9286  1342     5     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0]\n",
            "Target Masks: [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def data_generator(articles, summaries, batch_size, sp, max_length=512):\n",
        "    num_samples = len(articles)\n",
        "\n",
        "    while True:\n",
        "        for offset in range(0, num_samples, batch_size):\n",
        "            batch_articles = articles[offset:offset + batch_size]\n",
        "            batch_summaries = summaries[offset:offset + batch_size]\n",
        "\n",
        "            # Tokenize the batch data\n",
        "            tokenized_articles = [tokenize(article, max_length) for article in batch_articles]\n",
        "            tokenized_summaries = [tokenize(summary, max_length) for summary in batch_summaries]\n",
        "\n",
        "            # Pad tokenized sequences to ensure uniform shape\n",
        "            article_inputs = tf.keras.preprocessing.sequence.pad_sequences(\n",
        "                [t[0] for t in tokenized_articles], maxlen=max_length, padding='post'\n",
        "            )\n",
        "            article_masks = tf.keras.preprocessing.sequence.pad_sequences(\n",
        "                [t[1] for t in tokenized_articles], maxlen=max_length, padding='post'\n",
        "            )\n",
        "            summary_inputs = tf.keras.preprocessing.sequence.pad_sequences(\n",
        "                [t[0] for t in tokenized_summaries], maxlen=max_length, padding='post'\n",
        "            )\n",
        "            summary_masks = tf.keras.preprocessing.sequence.pad_sequences(\n",
        "                [t[1] for t in tokenized_summaries], maxlen=max_length, padding='post'\n",
        "            )\n",
        "\n",
        "            # Yield a tuple of inputs and a tuple of outputs\n",
        "            yield (\n",
        "                (article_inputs, article_masks),\n",
        "                (summary_inputs, summary_masks)\n",
        "            )\n"
      ],
      "metadata": {
        "id": "UIuCvH16x892"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "def data_generator(articles, summaries, batch_size, sp, max_length=256):\n",
        "    num_samples = len(articles)\n",
        "\n",
        "    while True:\n",
        "        for offset in range(0, num_samples, batch_size):\n",
        "            batch_articles = articles[offset:offset + batch_size]\n",
        "            batch_summaries = summaries[offset:offset + batch_size]\n",
        "\n",
        "            # Tokenize and ensure consistent max_length\n",
        "            tokenized_articles = [sp.encode_as_ids(article)[:max_length] for article in batch_articles]\n",
        "            tokenized_summaries = [sp.encode_as_ids(summary)[:max_length] for summary in batch_summaries]\n",
        "\n",
        "            # Pad tokenized sequences to ensure uniform shape\n",
        "            article_inputs = tf.keras.preprocessing.sequence.pad_sequences(\n",
        "                tokenized_articles, maxlen=max_length, padding='post', truncating='post'\n",
        "            )\n",
        "            summary_inputs = tf.keras.preprocessing.sequence.pad_sequences(\n",
        "                tokenized_summaries, maxlen=max_length, padding='post', truncating='post'\n",
        "            )\n",
        "\n",
        "            # Create masks (1 for non-padded tokens, 0 for padded)\n",
        "            article_masks = tf.cast(article_inputs != 0, tf.int32)\n",
        "            summary_masks = tf.cast(summary_inputs != 0, tf.int32)\n",
        "\n",
        "            # Yield a tuple of inputs and a tuple of outputs\n",
        "            yield (\n",
        "                (article_inputs, article_masks),\n",
        "                (summary_inputs, summary_masks)\n",
        "            )\n",
        "\n",
        "# Define output signature for the generator\n",
        "output_signature = (\n",
        "    (\n",
        "        tf.TensorSpec(shape=(None, max_length), dtype=tf.int32),  # article inputs\n",
        "        tf.TensorSpec(shape=(None, max_length), dtype=tf.int32)   # article masks\n",
        "    ),\n",
        "    (\n",
        "        tf.TensorSpec(shape=(None, max_length), dtype=tf.int32),  # summary inputs\n",
        "        tf.TensorSpec(shape=(None, max_length), dtype=tf.int32)   # summary masks\n",
        "    )\n",
        ")\n",
        "\n",
        "# Convert generator into a Dataset using output_signature\n",
        "dataset = tf.data.Dataset.from_generator(\n",
        "    lambda: data_generator(articles, summaries, batch_size, sp, max_length),\n",
        "    output_signature=output_signature\n",
        ")\n",
        "\n",
        "# Compile and train the model\n",
        "model = transformer_decoder_model(vocab_size=sp.vocab_size())\n",
        "model.compile(\n",
        "    optimizer='adam',\n",
        "    loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "\n",
        "    metrics=[tf.keras.metrics.SparseCategoricalAccuracy()]  # Add accuracy metric\n",
        ")\n",
        "\n",
        "# Define steps per epoch and validation steps\n",
        "steps_per_epoch = len(articles) // batch_size\n",
        "\n",
        "# Train the model using the dataset\n",
        "history = model.fit(\n",
        "    dataset,\n",
        "    steps_per_epoch=steps_per_epoch,\n",
        "    epochs=30\n",
        ")\n",
        "\n",
        "# Display training history\n",
        "print(history.history)  # This will show loss and accuracy after training\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7RIsy3iT0VUY",
        "outputId": "f63dcaad-4aba-44e4-c65c-d944d938e5db"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Split your data into training and validation sets\n",
        "train_articles, val_articles, train_summaries, val_summaries = train_test_split(\n",
        "    articles, summaries, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Create a training dataset\n",
        "train_dataset = create_tf_dataset(train_articles, train_summaries, batch_size=batch_size)\n",
        "\n",
        "# Create a validation dataset\n",
        "val_dataset = create_tf_dataset(val_articles, val_summaries, batch_size=batch_size)\n",
        "\n",
        "# Evaluate the model on the validation dataset\n",
        "val_steps = len(val_articles) // batch_size\n",
        "evaluation_results = model.evaluate(val_dataset, steps=val_steps)\n",
        "\n",
        "# Display evaluation metrics\n",
        "print(\"Evaluation results:\")\n",
        "for metric, value in zip(model.metrics_names, evaluation_results):\n",
        "    print(f\"{metric}: {value:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 517
        },
        "id": "9B1JmNGe7ni5",
        "outputId": "f31af514-5431-4ae5-92d8-351630bab39e"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Exception encountered when calling Functional.call().\n\n\u001b[1mInvalid input shape for input Tensor(\"functional_3_1/Cast:0\", shape=(None,), dtype=int32). Expected shape (None, None), but input has incompatible shape (None,)\u001b[0m\n\nArguments received by Functional.call():\n  • inputs=('tf.Tensor(shape=(None,), dtype=string)', 'tf.Tensor(shape=(None,), dtype=string)')\n  • training=False\n  • mask=('None', 'None')",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-21-fd19d6c16ddf>\u001b[0m in \u001b[0;36m<cell line: 16>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;31m# Evaluate the model on the validation dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mval_steps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_articles\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m//\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0mevaluation_results\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_steps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;31m# Display evaluation metrics\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    120\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m             \u001b[0;31m# `keras.config.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/models/functional.py\u001b[0m in \u001b[0;36m_adjust_input_rank\u001b[0;34m(self, flat_inputs)\u001b[0m\n\u001b[1;32m    242\u001b[0m                     \u001b[0madjusted\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m                     \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 244\u001b[0;31m             raise ValueError(\n\u001b[0m\u001b[1;32m    245\u001b[0m                 \u001b[0;34mf\"Invalid input shape for input {x}. Expected shape \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    246\u001b[0m                 \u001b[0;34mf\"{ref_shape}, but input has incompatible shape {x.shape}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Exception encountered when calling Functional.call().\n\n\u001b[1mInvalid input shape for input Tensor(\"functional_3_1/Cast:0\", shape=(None,), dtype=int32). Expected shape (None, None), but input has incompatible shape (None,)\u001b[0m\n\nArguments received by Functional.call():\n  • inputs=('tf.Tensor(shape=(None,), dtype=string)', 'tf.Tensor(shape=(None,), dtype=string)')\n  • training=False\n  • mask=('None', 'None')"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras import layers\n",
        "\n",
        "# Transformer model definition\n",
        "def transformer_decoder_model(vocab_size, d_model=512, num_heads=4, num_layers=16, dff=2048, dropout_rate=0.1):\n",
        "    inputs = layers.Input(shape=(None,), dtype=tf.int32)\n",
        "    masks = layers.Input(shape=(None,), dtype=tf.int32)\n",
        "\n",
        "    # Embedding layer\n",
        "    embedding = layers.Embedding(vocab_size, d_model)(inputs)\n",
        "\n",
        "    # Expand mask dimensions to [batch_size, 1, seq_length]\n",
        "    expanded_masks = layers.Lambda(lambda x: tf.expand_dims(x, axis=1))(masks)\n",
        "\n",
        "    # Transformer decoder layers\n",
        "    for _ in range(num_layers):\n",
        "        attn_output = layers.MultiHeadAttention(\n",
        "            num_heads=num_heads,\n",
        "            key_dim=d_model,\n",
        "            dropout=dropout_rate\n",
        "        )(embedding, embedding, attention_mask=expanded_masks)\n",
        "\n",
        "        # Residual connection + normalization\n",
        "        attn_output = layers.LayerNormalization(epsilon=1e-6)(attn_output + embedding)\n",
        "\n",
        "        # Feedforward network\n",
        "        ffn_output = layers.Dense(dff, activation='relu')(attn_output)\n",
        "        ffn_output = layers.Dense(d_model)(ffn_output)\n",
        "        embedding = layers.LayerNormalization(epsilon=1e-6)(ffn_output + attn_output)\n",
        "\n",
        "    outputs = layers.Dense(vocab_size)(embedding)\n",
        "    return tf.keras.Model(inputs=[inputs, masks], outputs=outputs)\n"
      ],
      "metadata": {
        "id": "7dMu0V97x9AN"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Tokenizer function (assuming sp is the SentencePiece model instance)\n",
        "def tokenize(text, max_length=100):\n",
        "    tokenized_input = sp.encode(text, out_type=int)\n",
        "\n",
        "    # Pad the input tokens if shorter than max_length\n",
        "    input_length = len(tokenized_input)\n",
        "    if input_length < max_length:\n",
        "        tokenized_input = np.pad(tokenized_input, (0, max_length - input_length), 'constant', constant_values=0)\n",
        "\n",
        "    # Create a mask (1 for actual tokens, 0 for padding)\n",
        "    input_mask = np.where(tokenized_input != 0, 1, 0)\n",
        "\n",
        "    # Reshape to 2D arrays (batch_size=1, sequence_length)\n",
        "    return np.array(tokenized_input).reshape(1, -1), np.array(input_mask).reshape(1, -1)\n",
        "\n",
        "# Convert predicted token indices back to a sentence\n",
        "def tokens_to_sentence(token_indices):\n",
        "    # Decode token indices back to text using the tokenizer (assuming sp is the SentencePiece model)\n",
        "    tokens = sp.decode(token_indices)\n",
        "    return tokens\n",
        "\n",
        "# Example input text\n",
        "input_text = \"\"\"IAAF launches fight against drugs. The IAAF - athletics' world governing body - has met anti-doping officials, coaches, and athletes to co-ordinate the fight against drugs in sport.\n",
        "\n",
        "Two task forces have been set up to examine doping and nutrition issues. It was also agreed that a programme to \"de-mystify\" the issue to athletes, the public, and the media was a priority. \"Nothing was decided to change things - it was more to have a forum of the stakeholders allowing them to express themselves,\" said an IAAF spokesman. \"Getting everyone together gave us a lot of food for thought.\" About 60 people attended Sunday's meeting in Monaco, including IAAF chief Lamine Diack and Namibian athlete Frankie Fredericks, now a member of the Athletes' Commission. \"I am very happy to see you all, members of the athletics family, respond positively to the IAAF call to sit together and discuss what more we can do in the fight against doping,\" said Diack. \"We are the leading Federation in this field and it is our duty to keep our sport clean.\" The two task forces will report back to the IAAF Council, at its April meeting in Qatar.\n",
        "\"\"\"\n",
        "\n",
        "# Tokenize the input sentence\n",
        "input_seq, input_mask = tokenize(input_text)\n",
        "\n",
        "# Predict using the model\n",
        "predicted_logits = model.predict([input_seq, input_mask])\n",
        "\n",
        "predicted_probs = tf.nn.softmax(predicted_logits, axis=-1)\n",
        "print(\"predicted_probs:\", predicted_probs[0])\n",
        "predicted_token_indices = np.argmax(predicted_probs, axis=-1)\n",
        "\n",
        "print(\"Predicted Probabilities argmax:\", predicted_token_indices)\n",
        "\n",
        "\n",
        "# Remove padding (if any) from the predicted token indices\n",
        "predicted_token_indices = predicted_token_indices[0].tolist()\n",
        "predicted_token_indices = [idx for idx in predicted_token_indices if idx != 0]  # Remove padding (token 0)\n",
        "\n",
        "# Convert predicted token indices to sentence\n",
        "predicted_sentence = tokens_to_sentence(predicted_token_indices)\n",
        "\n",
        "# Output the results\n",
        "print(\"Predicted Token Indices:\", predicted_token_indices)\n",
        "print(\"Predicted Summary:\", predicted_sentence)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WKC2tbZ0zP_X",
        "outputId": "2b0705ad-c60d-4141-c40c-e5902ebddb02"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 151ms/step\n",
            "predicted_probs: tf.Tensor(\n",
            "[[3.0360398e-01 9.4068389e-07 2.4188583e-04 ... 8.4235484e-07\n",
            "  1.0045437e-06 1.0406445e-06]\n",
            " [3.0358395e-01 9.4069367e-07 2.4191383e-04 ... 8.4245261e-07\n",
            "  1.0045846e-06 1.0407039e-06]\n",
            " [3.0357921e-01 9.4070771e-07 2.4191040e-04 ... 8.4240253e-07\n",
            "  1.0046725e-06 1.0406787e-06]\n",
            " ...\n",
            " [3.0360258e-01 9.4065274e-07 2.4190571e-04 ... 8.4232357e-07\n",
            "  1.0045371e-06 1.0406496e-06]\n",
            " [3.0353776e-01 9.4089864e-07 2.4192478e-04 ... 8.4254543e-07\n",
            "  1.0047346e-06 1.0408761e-06]\n",
            " [3.0361092e-01 9.4059601e-07 2.4189806e-04 ... 8.4229930e-07\n",
            "  1.0045035e-06 1.0406118e-06]], shape=(276, 32000), dtype=float32)\n",
            "Predicted Probabilities argmax: [[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\n",
            "Predicted Token Indices: []\n",
            "Predicted Summary: \n"
          ]
        }
      ]
    }
  ]
}